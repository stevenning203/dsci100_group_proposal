{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Financial Fraud of Firms\n",
    "## 1.0 Introduction\n",
    "\n",
    "Firms report financial statements for every quarter and annual period. These statements are first reported internally by the organization, which in turn must be audited by accredited auditing firms to verify that the reports are accurate. Firms may be incentivized to create fraud within their reports for financial benefits such as taxation and value inflation. Fraudulent activity can cause massive monetary loss for investors (Hajek 2017), government organizations (Lin 2015), and individuals (Lin 2015). Fraudulent activity is also increasing (Lin 2015). Market participants have lost over $500 billion in recent years due to fraud (Rezaee 2002). This marks growing importance for developing better ways of detecting and eliminating financial fraud.\n",
    "\n",
    "The question we want to answer is: <strong>How accurately can we predict if a firm is fraudulent based on its reported vs audited monetary spending discrepancies, amount of money involved in misstatements, and historical monetary loss?</strong> (How we chose these variables is in <strong>1.13</strong>)\n",
    "\n",
    "The dataset that will be used to answer this question is Audit Data. Audit Data contains risk factor classifications and methods used by the external auditing firm. It contains reports from 777 firms from 46 different listings and 14 different sector scores. (Hooda 2018) Each observation has 27 values associated with it. However, many of these variables were not described in relevant articles. Additionally, some of the variables are calculated values determined with external analysis. These two types of variables are omitted in this analysis. \n",
    "\n",
    "### Variable used in analysis:\n",
    "\n",
    "<ol>\n",
    "    <li>PARA_A : Discrepancy in planned spending (Rs crore) (Hooda 2018)</li>\n",
    "    <li>PARA_B : Discrepancy in unplanned spending (Rs crore) (Hooda 2018)</li>\n",
    "    <li><strong>TOTAL</strong> : Sum of PARA_A and PARA_B (Rs crore)</li>\n",
    "    <li><strong>Money_Value</strong> : Amount of money involved in misstatements (Hooda 2018)</li>\n",
    "    <li><strong>History</strong> : Average monetary loss by a firm in the last 10 years (Hooda 2018)</li>\n",
    "    <li><strong>Risk (predicted variable)</strong> : 1 if the firm has been determined to be fraudulent. 0 if not.</li>\n",
    "</ol>\n",
    "\n",
    "#### [Figure 1.0]. Rs crore means Indian Rupees (in crore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Methods & Preliminary Exploratory Data Analysis\n",
    "\n",
    "### 1.11 Process Overview\n",
    "1. Data wrangling (1.12)\n",
    "1. Determine appropriate variables\n",
    "2. Determine Value of k with V-fold cross-validation\n",
    "3. Create model using k\n",
    "4. Testing model on testing dataset \n",
    "5. Analyze accuracy with accuracy figure, majority classifier and confusion matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Wrangling\n",
    "\n",
    "First, we load the libraries that we need, which are tidyverse, tidymodels, repr, and GGally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(repr)\n",
    "library(GGally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to download the data from the URL. Since it is in .zip format, we have to download it before unzipping and then using read_csv(). The file \"audit_risk.csv\" is the one with the data that we want to use to create our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00475/audit_data.zip\"\n",
    "download.file(url, \"audit_data.zip\")\n",
    "unzip(\"audit_data.zip\")\n",
    "\n",
    "audit_risk_data_main <- read_csv(\"audit_data/audit_risk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only those columns listed in Table 1.0.\n",
    "\n",
    "Before we inspect the data, we should split it up into training and testing data so we are only using the training data during our training portion of the analysis.\n",
    "\n",
    "Our data set is not especially large, so the prop ratio should not be that high. Let's choose a value of 2/3 for a good amount of testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET THE SEED\n",
    "set.seed(18)\n",
    "### DO NOT CHANGE\n",
    "\n",
    "audit_risk_data_pre_split <- audit_risk_data_main %>% select(PARA_A, PARA_B, TOTAL, Money_Value, History, Risk)\n",
    "\n",
    "audit_risk_data_split <- initial_split(audit_risk_data_pre_split, prop = 2/3, strata = Risk)\n",
    "audit_risk_data_training <- training(audit_risk_data_split)\n",
    "audit_risk_data_testing <- testing(audit_risk_data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we should inspect some rows of the table to check for quality and tidyness. The data should be examined for NA values, untidy columns, untidy observations, and obvious trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(audit_risk_data_training, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.12]. Legend: See [Figure 1.0]\n",
    "\n",
    "The data appears to be tidy in the columns we can see. Each column is one variable, each row is one observation, and each cell has exactly one value.\n",
    "\n",
    "Out of these 12 rows, there are no NA values, but let's check the number of observations with NA in the data. Let's use na.omit() to omit rows with NA values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(na.omit(audit_risk_data_training))\n",
    "nrow(audit_risk_data_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one row with missing data. Let's omit it. Additionally, since we are predicting Risk through classification, we should convert that column to a factor. Let's also do the same for the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_training <- na.omit(mutate(audit_risk_data_training, Risk = as_factor(Risk)))\n",
    "audit_testing <- na.omit(mutate(audit_risk_data_testing, Risk = as_factor(Risk)))\n",
    "\n",
    "head(select(audit_training, Risk), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Exploring the Data through Tables to Inspect Trends and Determine Variables\n",
    "\n",
    "The variables we can consider for our model are:\n",
    "\n",
    "- PARA_A\n",
    "- PARA_B\n",
    "- TOTAL\n",
    "- Money_Value\n",
    "- History\n",
    "\n",
    "Let's examine any trends by looking at some rows of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_consider <- audit_training\n",
    "\n",
    "head(audit_consider, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.13]. Legend: See [Figure 1.0]\n",
    "\n",
    "From Table 1.13, it is clear that TOTAL, Money_Value, PARA_A, and PARA_B have a relationship with Risk, indicating that these could be possible variables to predict Risk.\n",
    "\n",
    "Another thing of notability is how TOTAL appears to be the sum of PARA_A and PARA_B. Let's examine this further by creating a new column called AB that is the sum of PARA_A and PARA_B. Then we should check how many rows have TOTAL and AB not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_total_ne_aplusb <- audit_consider %>%\n",
    "mutate(AB = PARA_A + PARA_B, equal = (abs(AB - TOTAL) <= 0.1)) %>%\n",
    "filter(equal == FALSE)\n",
    "\n",
    "head(audit_total_ne_aplusb, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.131]. Legend: See [Figure 1.0]\n",
    "\n",
    "PARA_A, PARA_B, and TOTAL all represent discrepancies in reported spending vs audited spending, which is valuable since misreported spending would likely indicate fraud. Since TOTAL includes both PARA_A and PARA_B, only TOTAL should be considered.\n",
    "\n",
    "#### Visualizing the Training Data to Detect Trends and Find Relevant Variables\n",
    "\n",
    "#### Money_Value\n",
    "\n",
    "Next let's look at Money_Value, another one of the variables that appeared to have a relationship with Risk. In Table 1.13, high Money_Value values correspond with a Risk value of 1. We can create two histograms, one with the distributions of Money_Value for observations of Risk = 1, and the other with the distributions of Money_Value for observations of Risk = 0.\n",
    "\n",
    "We can do this by using facet_grid() to create 2 separate histogram distribution plots of Money_Value for Risk == 0 and Risk == 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 7)\n",
    "\n",
    "money_value_plot <- audit_consider %>%\n",
    "ggplot(aes(x = Money_Value)) +\n",
    "facet_grid(. ~ Risk) +\n",
    "geom_histogram() +\n",
    "labs(x = \"Money_Value values\", y = \"Count\") +\n",
    "ggtitle(\"Money_Value distribution for Risk Values of 0 and 1 Respectively\") +\n",
    "theme(text = element_text(size = 20)) +\n",
    "scale_x_log10()\n",
    "\n",
    "money_value_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1.132]. Money_Value: Amount of Money involved in misstatements (Hooda 2018)\n",
    "\n",
    "From Figure 1.132, it's pretty clear that Money_Value values associated with a Risk value of 1 are much higher on average than Money_Value values associated with a Risk value of 0. The histogram for Risk == 1 is shifted farther to the right and has a center of around 10, while the histogram for Risk == 0 has a center of between 0.1 and 1.\n",
    "\n",
    "#### TOTAL\n",
    "\n",
    "We haven't done this for TOTAL yet, so let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 7)\n",
    "\n",
    "money_value_plot <- audit_consider %>%\n",
    "ggplot(aes(x = TOTAL)) +\n",
    "facet_grid(. ~ Risk) +\n",
    "geom_histogram() +\n",
    "labs(x = \"TOTAL (Rs in Crore)\", y = \"Count\") +\n",
    "ggtitle(\"TOTAL Distribution for Risk Values of 0 and 1 Respectively\") +\n",
    "theme(text = element_text(size = 20)) +\n",
    "scale_x_log10()\n",
    "\n",
    "money_value_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1.133]. TOTAL: Discrepencies in planned and unplanned spending\n",
    "\n",
    "From Figure 1.133, there also appears to be a trend. There are much more TOTAL values greater than 0 if Risk == 1.\n",
    "\n",
    "Let's create a mean, min, max, and sd table for each of the variables in Table 1.13, separated by Risk value, to further help determine if they would be a good candidate or not. To do this, we will bind 8 rows of data, each with their respective statistic label and statistic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_rows(filter(audit_consider, Risk == 0) %>%\n",
    "select(-Risk) %>%\n",
    "map_df(mean, na.rm = TRUE) %>%\n",
    "mutate(Stat = \"Mean\", Risk = 0) %>%\n",
    "bind_rows(audit_consider %>%\n",
    "         filter(Risk == 0) %>%\n",
    "         select(-Risk) %>%\n",
    "         map_df(sd, na.rm = TRUE) %>%\n",
    "         mutate(Stat = \"SD\", Risk = 0)),\n",
    "          \n",
    "filter(audit_consider, Risk == 0) %>%\n",
    "          select(-Risk) %>%\n",
    "          map_df(min, na.rm = TRUE) %>%\n",
    "          mutate(Stat = \"Min\", Risk = 0),\n",
    "          \n",
    "filter(audit_consider, Risk == 0) %>%\n",
    "          select(-Risk) %>%\n",
    "          map_df(max, na.rm = TRUE) %>%\n",
    "          mutate(Stat = \"Max\", Risk = 0),\n",
    "          \n",
    "          filter(audit_consider, Risk == 1) %>%\n",
    "          select(-Risk) %>%\n",
    "          map_df(min, na.rm = TRUE) %>%\n",
    "          mutate(Stat = \"Min\", Risk = 1),\n",
    "          \n",
    "filter(audit_consider, Risk == 1) %>%\n",
    "          select(-Risk) %>%\n",
    "          map_df(max, na.rm = TRUE) %>%\n",
    "          mutate(Stat = \"Max\", Risk = 1),\n",
    "\n",
    "filter(audit_consider, Risk == 1) %>%\n",
    "select(-Risk) %>%\n",
    "map_df(mean, na.rm = TRUE) %>%\n",
    "mutate(Stat = \"Mean\", Risk = 1) %>%\n",
    "bind_rows(audit_consider %>%\n",
    "         filter(Risk == 1) %>%\n",
    "         select(-Risk) %>%\n",
    "         map_df(sd, na.rm = TRUE) %>%\n",
    "         mutate(Stat = \"SD\", Risk = 1))) %>%\n",
    "mutate(Risk = as_factor(Risk), Stat = as_factor(Stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.134]. Legend: See [Figure 1.0]\n",
    "\n",
    "The means of TOTAL for Risk == 1 and Risk == 0 vary greatly. The mean TOTAL for Risk == 0 is about 1, but it is ~33 for Risk == 1. This supports our choice of TOTAL as a predictor variable because it indicates that TOTAL can be used to determine if a firm is fraudulent or not. Same with Money_Value. The mean for Risk == 0 is about 0.5, while the mean for Risk == 1 is about 43. Using the same reasoning, we can conclude that these two variables are good for our model.\n",
    "\n",
    "That leaves the variable History.\n",
    "\n",
    "#### History\n",
    "\n",
    "History could be valuable to our analysis because a fraudulent firm would likely suffer losses on record. The firm could commit fraud to prevent catastrophic loss. In addition, a potentially fraudulent firm would be less likely to have investors and business partners.\n",
    "\n",
    "Table 1.134 supports adding History as a variable. For Risk == 0, History has a mean of ~0.003 and does not vary by much, only having an SD of ~0.056. In contrast, History for Risk == 1 has a mean of ~0.28 and an SD of ~0.86. History score would be a good predictor to include in our model.\n",
    "\n",
    "We have considered all the possible predictors and chosen three for the model.\n",
    "\n",
    "#### Summary of Chosen Variables:\n",
    "\n",
    "1. History\n",
    "\n",
    "2. TOTAL\n",
    "\n",
    "3. Money_Value\n",
    "\n",
    "4. Risk (predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the analysis, let's make sure that the number of observations with Risk == 1 and Risk == 0 is somewhat proportional so that we have enough of both to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_risk_data_count <- audit_training %>%\n",
    "group_by(Risk) %>%\n",
    "summarize(n = n())\n",
    "\n",
    "audit_risk_data_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1.135]. Risk: If a firm has been determined to be fraudulent (1) or not (0). n: the amount of observations with the respective Risk value.\n",
    "\n",
    "There is a good ratio, so no upsampling has to be done.\n",
    "\n",
    "## 1.2 Starting the Analysis\n",
    "\n",
    "### 1.21 Creating our Specs and Recipes for Cross-Validation\n",
    "\n",
    "#### Specification\n",
    "\n",
    "We will use K nearest neighbors classification from tidymodels to create our classification model. Firstly, we have to find an optimum value of K by cross-validation for K = 1 to 50. After determining the new value of K, we will create a new spec that uses the final value of K instead of tune()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(999998)\n",
    "\n",
    "audit_knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "set_engine(\"kknn\") %>%\n",
    "set_mode(\"classification\")\n",
    "\n",
    "audit_knn_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipe\n",
    "\n",
    "Now we will create a recipe for our data. The predicted variable is Risk and the predictors are Money_Value, TOTAL, and History, so we put that as the first argument of recipe().\n",
    "\n",
    "We should standardize the data because the scales for the variables are very different. History lies about from 0 to 10 while the other variables can range from 0 to ~1200. Hence, let's add step_center() and step_scale()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(50)\n",
    "\n",
    "audit_training_recipe <- recipe(Risk ~ Money_Value + History + TOTAL, audit_training) %>%\n",
    "step_center(all_predictors()) %>%\n",
    "step_scale(all_predictors())\n",
    "\n",
    "audit_training_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vfold_cv cross-validation object\n",
    "\n",
    "Our final step before we combine these steps into a workflow is to create the vfold_cv object. We will use 5 folds and K values for 1 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1001)\n",
    "\n",
    "gridvals <- tibble(neighbors = 1:50)\n",
    "audit_vfold <- vfold_cv(audit_training, C = 5, strata = Risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.22 Tuning\n",
    "\n",
    "Let's start the tuning. First, we will combine the model and the recipe we created. Then we will use tune_grid() to perform the cross-validation for our vfold_cv object that we created earlier and our gridvals tibble from the code block directly above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1015)\n",
    "\n",
    "audit_training_fit <- workflow() %>%\n",
    "add_model(audit_knn_spec) %>%\n",
    "add_recipe(audit_training_recipe) %>%\n",
    "tune_grid(resamples = audit_vfold, grid = gridvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results to determine K. To plot the accuracy vs K, we will use collect_metrics() and then plot accuracy on the y-axis and K values on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(26)\n",
    "\n",
    "options(repr.plot.width = 16)\n",
    "\n",
    "audit_training_metrics <- collect_metrics(audit_training_fit) %>%\n",
    "filter(.metric == \"accuracy\")\n",
    "\n",
    "audit_training_k_plot <- audit_training_metrics %>%\n",
    "ggplot(aes(x = neighbors, y = mean)) +\n",
    "geom_point() +\n",
    "geom_line() +\n",
    "labs(x = \"K Value\", y = \"Mean Accuracy\") +\n",
    "theme(text = element_text(size = 20)) +\n",
    "ggtitle(\"Accuracy of Various K's in 5 fold Cross-Validation for Audit Training Data\") +\n",
    "scale_x_continuous(breaks = 2 * c(1:25))\n",
    "\n",
    "audit_training_k_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1.22]. K Value: The K value for each model. Mean Accuracy: The average accuracy of predicted vs truth in the cross-validation.\n",
    "\n",
    "Judging from the graph, it appears that we have an optimum K at 13 or 14. Let's pull the optimum K value and its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_final_k <- audit_training_metrics %>%\n",
    "arrange(desc(mean)) %>%\n",
    "select(neighbors) %>%\n",
    "head(1) %>%\n",
    "pull()\n",
    "\n",
    "paste(\"Final K Value:  K = \", audit_final_k)\n",
    "audit_final_k_acc <- audit_training_metrics %>%\n",
    "filter(neighbors == audit_final_k) %>%\n",
    "select(mean) %>%\n",
    "pull()\n",
    "\n",
    "paste(\"Accuracy: \", audit_final_k_acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrive at a K value of 13 with an accuracy of around 92.6%, which is fairly high.\n",
    "\n",
    "Let's create a new spec that uses the final K value.\n",
    "\n",
    "The spec and recipe are combined, and then the model is fit onto the training data, creating our final fit that we can use to predict Risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1009)\n",
    "\n",
    "audit_final_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = audit_final_k) %>%\n",
    "set_engine(\"kknn\") %>%\n",
    "set_mode(\"classification\")\n",
    "\n",
    "audit_final_fit <- workflow() %>%\n",
    "add_recipe(audit_training_recipe) %>%\n",
    "add_model(audit_final_spec) %>%\n",
    "fit(data = audit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Testing the Model on the Testing Data\n",
    "\n",
    "### 1.31 Predicting\n",
    "\n",
    "Our final model has been created with the training data. Let's predict our testing data and produce the table to see roughly how accurate our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(992)\n",
    "\n",
    "audit_tested <- bind_cols(audit_testing %>% select(Risk), predict(audit_final_fit, audit_testing))\n",
    "\n",
    "head(audit_tested, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.31]. Risk: The predicted class, whether or not a firm is considered fraudulent. .pred_class: the predicted class based on the model.\n",
    "\n",
    "From Table 1.31, it appears that our model has done a pretty good job of predicting whether or not a firm is fraudulent based on the predictor variables. Next, let's visualize our results.\n",
    "\n",
    "### 1.32 Results\n",
    "\n",
    "To determine the accuracy of our model, we can use metrics() on audit_tested. After using metrics(), let's filter for only accuracy since that's the only statistic we are interested in seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_final_metrics <- metrics(audit_tested, truth = Risk, estimate = .pred_class) %>%\n",
    "filter(.metric == \"accuracy\")\n",
    "\n",
    "audit_final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Table 1.32]. .metric: the type of metric measured. .estimator: binary because we had exactly two classes. .estimate: our accuracy.\n",
    "\n",
    "The final accuracy comes to ~93.8%.\n",
    "\n",
    "Let's investigate further by checking the confusion matrix to see if the model tended to falsely predict Risk == 0 as Risk == 1 or Risk == 1 as Risk == 0. We can use conf_mat with the same arguments as we did for Table 1.32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_final_mat <- conf_mat(audit_tested, truth = Risk, estimate = .pred_class)\n",
    "\n",
    "audit_final_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Figure 1.31]. Truth: Risk == 0 or Risk == 1. Prediction: Risk == 0 or Risk == 1. Diagonal: The amount of predictions that had a truth equal to truth and a prediction equal to prediction.\n",
    "\n",
    "Interestingly, it appears that the only predictions that were incorrect were false negatives. This could be explained by missing variables that correlate with fraud but were not included in our model.\n",
    "\n",
    "## 1.4 Outcomes and Significance\n",
    "\n",
    "### 1.41 Outcome Summary\n",
    "\n",
    "We discovered that the model we created was fairly accurate at a percentage accuracy of ~<strong>93.8%</strong>. All 16 of the false predictions in the model were false negatives, meaning that some firms were classified as Risk = 0 but the truth was Risk = 1.\n",
    "\n",
    "In our proposal, we hypothesized that the accuracy of the model would be \"high\" for the testing data. Our findings match this hypothesis. All our variables were numeric and related to money. Since detecting fraud cannot be fully determined only from monetary measurements, we expected that the accuracy would not be near 100%. The outcome matches that expectation.\n",
    "\n",
    "### 1.42 Impacts\n",
    "The model we developed can significantly reduce the time spent by auditors on gathering, correlating, formatting and summarizing information. Instead, they can start their analysis on the predicted fraudulent by our model and study the implications of these results. This will not only greatky increase their efficiency but also liberate auditors from repetitive work and transform their roles. \n",
    "\n",
    "Furthermore, COVID-19 and the resulting environment has changed how we work for the foreseeable future. We can expect more work done digitally, providing more information that can be incorporated into training better models that help with identifing fraudulent cases. The process of which this model is created is reproducable and can be easily edited to add more predictors. Therefore, we can expect the model to have continuing contributions. \n",
    "\n",
    "### 1.43 Further Exploration \n",
    "\n",
    "* How could we implement non-numeric variables in our classification?\n",
    "* Is there a way to use regression such that we could get a Risk decimal to determine which firms are most likely to be fraudulent and prioritize investigating those?\n",
    "* Is it advantageous to only use measured variables for machine models detecting fraud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 References\n",
    "\n",
    "Audit Data Set: https://archive.ics.uci.edu/ml/datasets/Audit+Data\n",
    "\n",
    "Citation Request for Audit Data Set: \"This research work is supported by Ministry of Electronics and Information Technology (MEITY), Govt.of India\"\n",
    "\n",
    "Reference format: [First name, Middle name(s), Last name. First Name, Middle name(s), Last Name...etc.] Title. Year of publication. \\<link to article\\>\n",
    "\n",
    "Citation format: (\\<Last name of first author\\> \\<Year of publication\\>)\n",
    "\n",
    "[Chi-Chen, Lin. An-An, Chiu. Shaio, Yan, Huang. David, C., Yen.] Detecting the financial statement fraud: The analysis of the differences between data mining techniques and experts’ judgments. 2015. https://doi.org/10.1016/j.knosys.2015.08.011\n",
    "\n",
    "[Nishtha, Hooda. Seema, Bawa. Prashant, Singh, Rana.] Fraudulent Firm Classification: A Case Study of an External Audit. 2018. https://doi.org/10.1080/08839514.2018.1451032\n",
    "\n",
    "=> The above article is paywalled. Non pay-walled link: <a href = \"https://www.researchgate.net/publication/323655455_Fraudulent_Firm_Classification_A_Case_Study_of_an_External_Audit\">HERE</a>\n",
    "\n",
    "[Petr, Hajek. Roberto, Henriques.] Mining corporate annual reports for intelligent detection of financial statement fraud – A comparative study of machine learning methods. 2017. https://doi.org/10.1016/j.knosys.2017.05.001\n",
    "\n",
    "[Zabihollah, Rezaee.] Causes, consequences, and deterence of financial statement fraud. 2002.\n",
    "https://doi.org/10.1016/S1045-2354(03)00072-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
